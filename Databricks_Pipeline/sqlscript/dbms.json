{
	"name": "dbms",
	"properties": {
		"content": {
			"query": "--\n-- Schemas - stage, active & analytics\n-- There are three schemas named active, stage and analytics. \n--\n\n-- Drop schema\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.schemas\n\t\tWHERE name = 'stage'\n\t\t)\n\tDROP SCHEMA [stage]\nGO\n\n-- Add schema\nCREATE SCHEMA [stage] AUTHORIZATION [dbo]\nGO\n\n-- Drop schema\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.schemas\n\t\tWHERE name = 'active'\n\t\t)\n\tDROP SCHEMA [active]\nGO\n\n-- Add schema\nCREATE SCHEMA [active] AUTHORIZATION [dbo]\nGO\n\n-- Drop schema\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.schemas\n\t\tWHERE name = 'analytics'\n\t\t)\n\tDROP SCHEMA [analytics]\nGO\n\n-- Add schema\nCREATE SCHEMA [analytics] AUTHORIZATION [dbo]\nGO\n\n\n\n\n-- The customer acquistion data table in the stage schema is defined with variable length character fields. \n-- That way, the insertion of data from a Synapse Pipeline copy activity never fails. \n\n-- Drop stage table\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.objects\n\t\tWHERE object_id = OBJECT_ID(N'[stage].[customer_acquistion_data]')\n\t\t\tAND type IN (N'U')\n\t\t)\n\tDROP TABLE [stage].[customer_acquistion_data]\nGO\n\n-- Create stage table\nCREATE TABLE [stage].[customer_acquistion_data] (\n\t[customer_id] VARCHAR(64) NULL\n\t,[relationship_manager_id] VARCHAR(64) NULL\n\t,[last_updated] VARCHAR(64) NULL\n\t,[deposit_amount] VARCHAR(64) NULL\n\t)\nWITH ( CLUSTERED COLUMNSTORE INDEX )\nGO\n\n\n\n\n--\n-- Table - current watermark\n-- The watermark table keeps track of the last run date.\n\n-- Drop stage table if it exists\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.objects\n\t\tWHERE object_id = OBJECT_ID(N'[stage].[current_watermark]')\n\t\t\tAND type IN (N'U')\n\t\t)\n\tDROP TABLE [stage].[current_watermark]\nGO\n\n-- Create stage table\nCREATE TABLE [stage].[current_watermark] ([process_date_hour] DATETIME NOT NULL)\nGO\n\n-- Clear any existing data\nTRUNCATE TABLE [stage].[current_watermark]\nGO\n\n-- Add data to control table\nINSERT INTO [stage].[current_watermark]\nVALUES ('20210101 00:00:00')\nGO\n\n\n\n\n\n--\n-- Table - active customer acquisition data\n-- On the other hand, the customer acquistion data table in the active schema is correctly typed. \n\n-- Drop active table\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.objects\n\t\tWHERE object_id = OBJECT_ID(N'[active].[customer_acquistion_data]')\n\t\t\tAND type IN (N'U')\n\t\t)\n\tDROP TABLE [active].[customer_acquistion_data]\nGO\n\n-- Create active table\nCREATE TABLE [active].[customer_acquistion_data] (\n\t[customer_id] INT NOT NULL\n\t,[relationship_manager_id] INT NOT NULL\n\t,[last_updated] DATE NOT NULL\n\t,[deposit_amount] DECIMAL NOT NULL\n\t)\nWITH  \n  (   \n    DISTRIBUTION = HASH (customer_id),  \n    CLUSTERED COLUMNSTORE INDEX\n  )  \nGO\n\n\n\n--\n-- Stored Proc. - Increment water mark\n-- The increment watermark stored procedure updates the process date by one day each time it is called. \n-- Please see the code below for the definition.\n--\n-- Drop procedure\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.procedures\n\t\tWHERE name = 'increment_watermark'\n\t\t)\n\tDROP PROCEDURE [stage].[increment_watermark]\nGO\n\n-- Create procedure\nCREATE PROCEDURE [stage].[increment_watermark]\nAS\nBEGIN\n\tUPDATE [stage].[current_watermark]\n\tSET [process_date_hour] = DATEADD(dd, 1, [process_date_hour])\nEND\nGO\n\n\n\n\n\n\n--\n-- View - formatted stage data\n-- The cleaned customer acquistion data view tries to cast the variable length character data in the \n-- stage table into strongly typed fields that are compatible with the target table.\n\n-- Drop view\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.objects\n\t\tWHERE name = OBJECT_ID(N'[stage].[vw_cleaned_customer_acquistion_data]')\n\t\t)\n\tDROP VIEW [stage].[vw_cleaned_customer_acquistion_data]\nGO\n\n-- Create view\nCREATE VIEW [stage].[vw_cleaned_customer_acquistion_data]\nAS\nSELECT try_cast([customer_id] AS INT) AS customer_id\n\t,try_cast([relationship_manager_id] AS INT) AS relationship_manager_id\n\t,try_cast([last_updated] AS DATE) AS [date]\n\t,try_cast([deposit_amount] AS DECIMAL) AS deposit_amount\nFROM [stage].[customer_acquistion_data]\nGO\n\n--\n-- Stored Proc. - upsert customer acquisition data\n-- The upsert customer acquisition data stored procedure uses the MERGE statement \n-- to execute an update on matching records and an insert for unmatched records.\n\n-- Drop procedure\nIF EXISTS (\n\t\tSELECT *\n\t\tFROM sys.procedures\n\t\tWHERE name = 'upsert_customer_acquistion_data'\n\t\t)\n\tDROP PROCEDURE [stage].[upsert_customer_acquistion_data]\nGO\n\n-- Create procedure\nCREATE PROCEDURE [stage].[upsert_customer_acquistion_data]\nAS\nBEGIN\n\t-- Set no count\n\tSET NOCOUNT ON\n\n\t-- Merge the clean stage data with active table\n\tMERGE [active].[customer_acquistion_data] AS trg\n\tUSING (\n\t\tSELECT *\n\t\tFROM [stage].[vw_cleaned_customer_acquistion_data]\n\t\t) AS src\n\t\tON src.[date] = trg.[last_updated]\n\t\t\tAND src.[relationship_manager_id] = trg.[relationship_manager_id]\n\t\t\tAND src.[customer_id] = trg.[customer_id]\n\t\t\t-- Update condition\n\tWHEN MATCHED\n\t\tTHEN\n\t\t\tUPDATE\n\t\t\tSET [customer_id] = src.[customer_id]\n\t\t\t\t,[relationship_manager_id] = src.[relationship_manager_id]\n\t\t\t\t,[last_updated] = src.[date]\n\t\t\t\t,[deposit_amount] = src.[deposit_amount]\n\t\t\t\t-- Insert condition\n\tWHEN NOT MATCHED BY TARGET\n\t\tTHEN\n\t\t\tINSERT (\n\t\t\t\t[customer_id]\n\t\t\t\t,[relationship_manager_id]\n\t\t\t\t,[last_updated]\n\t\t\t\t,[deposit_amount]\n\t\t\t\t)\n\t\t\tVALUES (\n\t\t\t\tsrc.[customer_id]\n\t\t\t\t,src.[relationship_manager_id]\n\t\t\t\t,src.[date]\n\t\t\t\t,src.[deposit_amount]\n\t\t\t\t);\nEND\nGO\n\n\n\n/*    \n    Show database objects\n*/\nSELECT *\nFROM sys.objects\nWHERE is_ms_shipped = 0\nORDER BY [name];\n\n\n\n\n-- In a nutshell, the database schema is setup to process new customer data every day. \n-- This new data in the stage schema is upserted into the final table in the active schema. \n-- The watermark table allows the ETL program to be restarted to a prior date and hour at will. \n-- Now that we have the relational database schema worked out, we can focus on designing an Azure Synapse Pipeline\n-- to automate the reading of data from data lake storage and the writing of data to the final active table.\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "GlobomanticsDWH",
				"poolName": "GlobomanticsDWH"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}